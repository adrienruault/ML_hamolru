

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot = True)

# To be changed to 2
NUM_CLASSES = 10

# To be changed to 400, 400
IMG_WIDTH = 28
IMG_HEIGHT = 28

NUM_EPOCHS = 2

# To be changed to 3
NUM_CHANNELS = 1

# To be changed to 1
BATCH_SIZE = 128


# These values need to be fed in the sess.run() function using feed_dict
x = tf.placeholder('float', [None, 784])
y = tf.placeholder('float')


def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding = 'SAME')


# ksize defines the size of the pool window
# strides defines the way the window moves (movement of the window)
def maxpool2d(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')


KEEP_RATE = 0.8
KEEP_PROB = tf.placeholder(tf.float32)


# data must be a 4D tensor generated by tf.constant()
# applied on a 4D numpy array with [image index, IMAGE_WIDTH, IMAGE_HEIGHT, CHANNEL]
def FCN_model(data):

    # Could use tf.truncated_normal() instead of tf.random_normal(),
    # see what is the best choice
    # note tf.truncated() is used in the FCN implementation
    weights = {'W_conv1': tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 32])),
               'W_conv2': tf.Variable(tf.truncated_normal([5, 5, 32, 64])),
               # Number of pools done until now: 2 -> image size is (IMG_WIDTH / 4 * IMG_HEIGHT / 4)
               'W_fc': tf.Variable(tf.truncated_normal([int(IMG_WIDTH / 4 * IMG_HEIGHT / 4) * 64, 1024])), # 7*7*64 is the dimension of the lower layer after reshaping (28 by 28 images)
               'out': tf.Variable(tf.truncated_normal([1024, NUM_CLASSES]))}

    biases = {'B_conv1': tf.Variable(tf.truncated_normal([32])),
               'B_conv2': tf.Variable(tf.truncated_normal([64])),
               'B_fc': tf.Variable(tf.truncated_normal([1024])), # 7*7*64 is the dimension of the lower layer after reshaping (28 by 28 images)
               'out': tf.Variable(tf.truncated_normal([NUM_CLASSES]))}

    # IMPORTANT STEP
    # From FCN implementation:
    # shape = tf.shape(data)
    # deconv_shape3 = tf.stack([shape[0], shape[1], shape[2], NUM_OF_CLASSESS])
    data = tf.reshape(data, shape=[-1, IMG_WIDTH, IMG_HEIGHT, NUM_CHANNELS])

    conv1 = conv2d(data, weights['W_conv1'])
    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, biases['B_conv1']))
    pool1 = maxpool2d(relu1)

    conv2 = conv2d(pool1, weights['W_conv2'])
    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, biases['B_conv2']))
    pool2 = maxpool2d(conv2)

    fc = tf.reshape(pool2, [-1, int(IMG_WIDTH / 4 * IMG_HEIGHT / 4) * 64])
    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'] + biases['B_fc']))

    dropout = tf.nn.dropout(fc, KEEP_RATE)

    output = tf.matmul(fc, weights['out'] + biases['out'])

    return output

def train_neural_network(x):
    # the prediction variable is often called logits in tensor flow vocabulary
    prediction = FCN_model(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels = y) )
    optimizer = tf.train.AdamOptimizer().minimize(cost)

    hm_epochs = NUM_EPOCHS
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for epoch in range(hm_epochs):
            epoch_loss = 0
            for _ in range(int(mnist.train.num_examples/BATCH_SIZE)):
                # feeding epoch_x and epoch_y for training current batch (to replace with our own )
                epoch_x, epoch_y = mnist.train.next_batch(BATCH_SIZE)
                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})
                epoch_loss += c

            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)

        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))

        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))

train_neural_network(x)
